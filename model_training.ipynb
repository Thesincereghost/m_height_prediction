{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ccc6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 17:02:54.602736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744149775.459392     522 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744149775.656451     522 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744149777.378623     522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744149777.378650     522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744149777.378652     522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744149777.378653     522 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 17:02:57.476076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9640c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba7ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel:\n",
    "    def __init__(self, n, k, max_m_value):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.max_m_value = max_m_value\n",
    "\n",
    "        self.P_matrices = None\n",
    "        self.m_heights = None\n",
    "        self.P_matrices_train = None\n",
    "        self.P_matrices_test = None\n",
    "        self.P_matrices_val = None\n",
    "        self.m_heights_train = None\n",
    "        self.m_heights_test = None\n",
    "        self.m_heights_log_train = None\n",
    "        self.m_heights_log_test = None\n",
    "        self.m_heights_log_val = None\n",
    "        self.eval_inputs = None\n",
    "        self.eval_outputs = None\n",
    "        self.P_matrices_train_aug = None\n",
    "        self.m_heights_log_train_aug = None\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "        self.model_name = f'model_{self.n}_{self.k}.keras'\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    def load_and_preprocess_data(self,filepath):\n",
    "        with gzip.open(filepath, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "\n",
    "        self.P_matrices = []\n",
    "        self.m_heights = []\n",
    "\n",
    "        for row in dataset:\n",
    "            # Remove the last `inf`, this is an error due to bug in the dataset generation\n",
    "            heights = row['m_heights'][:self.max_m_value]\n",
    "            if len(heights) != self.max_m_value:\n",
    "                continue\n",
    "            if any(h == float('inf') or h <= 0 for h in heights):\n",
    "                continue\n",
    "            # Convert m-heights to log scale\n",
    "            # log_heights = np.log(heights)\n",
    "            P_matrix = np.array(row['P_matrix'], dtype=np.float32)\n",
    "\n",
    "            self.P_matrices.append(P_matrix)\n",
    "            # m_heights.append(log_heights)\n",
    "            self.m_heights.append(heights)\n",
    "        self.P_matrices = np.array(self.P_matrices, dtype=np.float32)\n",
    "        self.m_heights = np.array(self.m_heights, dtype=np.float32)\n",
    "        print(\"P_matrices shape:\", self.P_matrices.shape)\n",
    "        print(\"m_heights shape:\", self.m_heights.shape)\n",
    "        # pritnt range of all m-heights\n",
    "        print(\"Range of m_heights:\")\n",
    "        print(\"Min:\", np.min(self.m_heights, axis=0))\n",
    "        print(\"Max:\", np.max(self.m_heights, axis=0))\n",
    "\n",
    "    def generate_permuted_dataset(self, P_matrices, m_heights, num_permutations=3):\n",
    "        \"\"\"Create augmented dataset by permuting columns of P_matrices.\"\"\"\n",
    "        augmented_P_matrices = []\n",
    "        augmented_m_heights = []\n",
    "\n",
    "        for i in range(len(P_matrices)):\n",
    "            original = P_matrices[i]\n",
    "            target = m_heights[i]\n",
    "\n",
    "            # Keep original\n",
    "            augmented_P_matrices.append(original)\n",
    "            augmented_m_heights.append(target)\n",
    "\n",
    "            # Generate random permutations\n",
    "            for _ in range(num_permutations):\n",
    "                permuted = original[:, np.random.permutation(original.shape[1])]\n",
    "                augmented_P_matrices.append(permuted)\n",
    "                augmented_m_heights.append(target)\n",
    "        return np.array(augmented_P_matrices), np.array(augmented_m_heights)\n",
    "    def train_test_val_split(self, val_split=0.2, test_split=0.2, num_permutations=4, batch_size=128, random_state=2342):\n",
    "\n",
    "        if(self.P_matrices is None or self.m_heights is None):\n",
    "            raise ValueError(\"Data not loaded. Please load the data first.\")\n",
    "        # Train-test split\n",
    "        self.P_matrices_train, self.P_matrices_test,\\\n",
    "              self.m_heights_train, self.m_heights_test\\\n",
    "                 = train_test_split(self.P_matrices, self.m_heights, test_size=test_split, random_state=random_state)\n",
    "\n",
    "        # Converting test set to evaluator format\n",
    "        self.eval_inputs = {}\n",
    "        self.eval_outputs = {}\n",
    "\n",
    "        # Iterate through each P_matrix and corresponding m_heights in test_dataset\n",
    "        for i in range(len(self.P_matrices_test)):\n",
    "            P_matrix = self.P_matrices_test[i]\n",
    "            m_heights_list = self.m_heights_test[i]\n",
    "            for m in range(2, self.max_m_value + 1):  # m ranges from 2 to max_m_value\n",
    "                key = f\"[{self.n},{self.k},{m}]\"\n",
    "\n",
    "                # Add the P_matrix to the inputs dictionary\n",
    "                if key not in self.eval_inputs:\n",
    "                    self.eval_inputs[key] = []\n",
    "                self.eval_inputs[key].append(P_matrix)\n",
    "\n",
    "                # Add the corresponding m_height to the outputs dictionary\n",
    "                if key not in self.eval_outputs:\n",
    "                    self.eval_outputs[key] = []\n",
    "                self.eval_outputs[key].append(m_heights_list[m - 1])  # m-1 to get the correct index\n",
    "\n",
    "        # Change m_heights_train and m_heights_test to log_scale\n",
    "        self.m_heights_log_train = np.log(self.m_heights_train)\n",
    "        self.m_heights_log_test = np.log(self.m_heights_test)\n",
    "        print(\"Range of log m_heights in training data:\")\n",
    "        print(\"Min:\", np.min(self.m_heights_log_train, axis=0))\n",
    "        print(\"Max:\", np.max(self.m_heights_log_train, axis=0))\n",
    "\n",
    "        # Train-validation split\n",
    "        self.P_matrices_train, self.P_matrices_val, self.m_heights_log_train, self.m_heights_log_val = train_test_split(self.P_matrices_train, self.m_heights_log_train, test_size=val_split, random_state=random_state)\n",
    "        # Generate augmented  training dataset\n",
    "        self.P_matrices_train_aug, self.m_heights_log_train_aug = self.generate_permuted_dataset(self.P_matrices_train, self.m_heights_log_train, num_permutations=num_permutations)\n",
    "        # Create TensorFlow datasets\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices((self.P_matrices_train_aug, self.m_heights_log_train_aug))\n",
    "        self.train_dataset = self.train_dataset.shuffle(self.P_matrices_train_aug.shape[0]).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.val_dataset = tf.data.Dataset.from_tensor_slices((self.P_matrices_val, self.m_heights_log_val))\n",
    "        self.val_dataset = self.val_dataset.shuffle(self.P_matrices_val.shape[0]).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        print(\"Train dataset shape:\", self.P_matrices_train_aug.shape, self.m_heights_log_train_aug.shape)\n",
    "        print(\"Validation dataset shape:\", self.P_matrices_val.shape, self.m_heights_log_val.shape)\n",
    "        print(\"Test dataset shape:\", self.P_matrices_test.shape, self.m_heights_log_test.shape)\n",
    "\n",
    "    def model_compile(self, build_model, print_summary=False):\n",
    "        if self.train_dataset is None or self.val_dataset is None:\n",
    "            raise ValueError(\"Train and validation datasets not created. Please create them first.\")\n",
    "        self.model = build_model(self.n, self.k, self.max_m_value)\n",
    "        print(\"Model compiled.\")\n",
    "        if print_summary:\n",
    "            self.model.summary()\n",
    "\n",
    "    def model_train(self, epochs=10, verbose=0, patience=5 , model_save_directory=\".\"):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Please build the model first.\")\n",
    "\n",
    "        # Early stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_save_directory + '/' + self.model_name, save_best_only=True)\n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            validation_data=self.val_dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                early_stopping,\n",
    "                model_checkpoint,\n",
    "            ],\n",
    "            verbose=verbose\n",
    "        )\n",
    "    def plot_history(self):\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model not trained. Train the model first.\")\n",
    "        history_dict = self.history.history\n",
    "        mse = history_dict[\"loss\"]\n",
    "        val_mse = history_dict[\"val_loss\"]\n",
    "        epochs = range(1, len(mse) + 1)\n",
    "        print('Train MSE :', mse)\n",
    "        print('Validation MSE :', val_mse)\n",
    "        plt.plot(epochs, mse, \"bo\", label=\"Training mse\")\n",
    "        plt.plot(epochs, val_mse, \"b\", label=\"Validation mse\")\n",
    "        plt.title(\"Training and validation mean squared error\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def evaluate_model(self, batch_size=128):\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model not trained. Train the model first.\")\n",
    "        results = self.model.evaluate(self.P_matrices_test, self.m_heights_log_test, batch_size=batch_size, verbose=0)\n",
    "        print('Best model loss :', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f394f4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict =  {\n",
    "    \"model_1\": {'n': 9, 'k': 4, 'max_m_value': 5, 'model': None, 'file_name': \"G_9_4_maxM5.pklgz\"},\n",
    "    \"model_2\": {'n': 9, 'k': 5, 'max_m_value': 4, 'model': None, 'file_name': \"G_9_5_maxM4.pklgz\"},\n",
    "    \"model_3\": {'n': 9, 'k': 6, 'max_m_value': 3, 'model': None, 'file_name': \"G_9_6_maxM3.pklgz\"},\n",
    "    \"model_4\": {'n': 10, 'k': 4, 'max_m_value': 6, 'model': None, 'file_name': \"G_10_4_maxM6.pklgz\"},\n",
    "    \"model_5\": {'n': 10, 'k': 5, 'max_m_value': 5, 'model': None, 'file_name': \"G_10_5_maxM5.pklgz\"},\n",
    "    \"model_6\": {'n': 10, 'k': 6, 'max_m_value': 4, 'model': None, 'file_name': \"G_10_6_maxM6.pklgz\"},\n",
    "}\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n, k, max_m_value):\n",
    "    input_shape = (k, n - k)  \n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Transpose so we can treat each column independently\n",
    "    # After transpose: shape becomes (batch_size, n-k, k)\n",
    "    transposed = layers.Permute((2, 1))(input_layer)  \n",
    "\n",
    "    # Shared MLP applied to each column (like DeepSets)\n",
    "    shared_mlp = tf.keras.Sequential([\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "    ])\n",
    "\n",
    "    # Apply shared MLP to each column (TimeDistributed over n-k columns)\n",
    "    encoded_columns = layers.TimeDistributed(shared_mlp)(transposed)  # (batch, n-k, 64)\n",
    "\n",
    "    # Aggregate over columns to achieve    \n",
    "    # avg = layers.GlobalAveragePooling1D()(encoded_columns)\n",
    "    # max_ = layers.GlobalMaxPooling1D()(encoded_columns)\n",
    "    # aggregated = layers.Concatenate()([avg, max_])\n",
    "    aggregated = layers.GlobalAveragePooling1D()(encoded_columns)  # (batch, 64)oded_columns)  # (batch, 64)\n",
    "\n",
    "    # Final prediction head\n",
    "    output_layer = layers.Dense(max_m_value, activation='linear')(aggregated)\n",
    "\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # model.compile(optimizer='adam', loss=tf.keras.losses.Huber())\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac69248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/mnt/d/MS/deep_learning/m_height_prediction\")\n",
    "samples_filepath = \"/mnt/d/MS/deep_learning/m_height_prediction/samples_combined/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf06c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for model_9_4...\n",
      "P_matrices shape: (34999, 4, 5)\n",
      "m_heights shape: (34999, 5)\n",
      "Range of m_heights:\n",
      "Min: [   3.3710756    8.660064    75.08341    182.32542   1253.2301   ]\n",
      "Max: [2.7675543e+02 7.5315314e+02 4.8695015e+03 2.2775238e+05 7.9118080e+08]\n",
      "\n",
      "Generating dataset for model_9_4...\n",
      "Range of log m_heights in training data:\n",
      "Min: [1.2152319 2.1587222 4.3185997 5.350549  7.1334796]\n",
      "Max: [ 5.623134   6.2349315  8.4907465 12.336015  20.489037 ]\n",
      "Train dataset shape: (134394, 4, 5) (134394, 5)\n",
      "Validation dataset shape: (5600, 4, 5) (5600, 5)\n",
      "Test dataset shape: (7000, 4, 5) (7000, 5)\n",
      "\n",
      "Compiling model for model_9_4...\n",
      "Model compiled.\n",
      "\n",
      "Training model for model_9_4...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# plot_model(model_info['model'].model, show_shapes=True, show_layer_names=True)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining model for model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_verbosity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating model for model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m model_info[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].plot_history()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mCustomModel.model_train\u001b[39m\u001b[34m(self, epochs, verbose, patience, model_save_directory)\u001b[39m\n\u001b[32m    142\u001b[39m model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_save_directory + \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mself\u001b[39m.model_name, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfgpu/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfgpu/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfgpu/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfgpu/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfgpu/lib/python3.12/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "model_save_directory = \"model_6\"\n",
    "val_split = 0.2\n",
    "test_split = 0.2\n",
    "num_permutations = 5\n",
    "batch_size = 128\n",
    "random_state = 4542\n",
    "epochs = 40\n",
    "patience = 5\n",
    "train_verbosity = 0\n",
    "for model_name, model_info in models_dict.items():\n",
    "    # if model_name != \"model_1\":\n",
    "    #     break\n",
    "    filepath = samples_filepath + model_info['file_name']\n",
    "    n = model_info['n']\n",
    "    k = model_info['k']\n",
    "    max_m_value = model_info['max_m_value']\n",
    "    model_info['model'] = CustomModel(n=n, k=k, max_m_value=max_m_value)\n",
    "    print(f\"Loading data for model_{n}_{k}...\")\n",
    "    model_info['model'].load_and_preprocess_data(filepath)\n",
    "    print(f\"\\nGenerating dataset for model_{n}_{k}...\")\n",
    "    model_info['model'].train_test_val_split(val_split=val_split, test_split=test_split, num_permutations=num_permutations, batch_size=batch_size, random_state=random_state)\n",
    "    print(f\"\\nCompiling model for model_{n}_{k}...\")\n",
    "    model_info['model'].model_compile(build_model, print_summary=False)\n",
    "    # plot_model(model_info['model'].model, show_shapes=True, show_layer_names=True)\n",
    "    print(f\"\\nTraining model for model_{n}_{k}...\")\n",
    "    model_info['model'].model_train(epochs=epochs, verbose=train_verbosity, patience=patience, model_save_directory = model_save_directory)\n",
    "    print(f\"\\nEvaluating model for model_{n}_{k}...\")\n",
    "    model_info['model'].plot_history()\n",
    "    print(f\"\\nPerformance on test set for model_{n}_{k}:\")\n",
    "    model_info['model'].evaluate_model(batch_size=batch_size)\n",
    "    print(f\"\\nDone with model_{n}_{k}.\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "eval_inputs_combined = {}\n",
    "eval_outputs_combined = {}\n",
    "\n",
    "for model_name, model_info in models_dict.items():\n",
    "    # if model_name != \"model_1\":\n",
    "    #     break\n",
    "    for key in model_info['model'].eval_inputs.keys():\n",
    "        if key not in eval_inputs_combined:\n",
    "            eval_inputs_combined[key] = []\n",
    "        eval_inputs_combined[key].extend(model_info['model'].eval_inputs[key])\n",
    "    for key in model_info['model'].eval_outputs.keys():\n",
    "        if key not in eval_outputs_combined:\n",
    "            eval_outputs_combined[key] = []\n",
    "        eval_outputs_combined[key].extend(model_info['model'].eval_outputs[key])\n",
    "\n",
    "with gzip.open(model_save_directory+'/eval_inputs_combined', 'wb') as f:\n",
    "            pickle.dump(eval_inputs_combined, f)\n",
    "with gzip.open(model_save_directory+'/eval_outputs_combined', 'wb') as f:\n",
    "            pickle.dump(eval_outputs_combined, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84a5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tamu_csce_636_project1 import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    first_name=\"Your Name\",\n",
    "    last_name=\"Your Name\",\n",
    "    email=\"email@tamu.edu\",\n",
    "    print=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d426d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(n,k,m,P_matrices):\n",
    "    ## load model\n",
    "    model_name = model_save_directory + '/' + f'model_{n}_{k}.keras'\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    m_heights = []\n",
    "    P_matrices = np.array(P_matrices, dtype=np.float32)\n",
    "    m_heights_pred = model.predict(P_matrices.reshape(len(P_matrices), k, n-k), verbose=0)\n",
    "    m_heights =[np.exp(m_heights_pred[i][m-1]) for i in range(len(P_matrices))]\n",
    "    # print(P_matrices[0])\n",
    "    # print(m_heights_pred[0])\n",
    "    # print(m_heights_pred[0][m-1])\n",
    "    # print(np.exp(m_heights_pred[0][m-1]))\n",
    "    # print(m_heights[0])\n",
    "    return m_heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9770c038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average σ: 1.324131338445433\n",
      "(9, 4, 2), σ: 0.18715476713098955\n",
      "(9, 4, 3), σ: 0.235722698718727\n",
      "(9, 4, 4), σ: 0.8122218849954684\n",
      "(9, 4, 5), σ: 3.296239620010989\n",
      "(9, 5, 2), σ: 0.21686554545692777\n",
      "(9, 5, 3), σ: 0.7954697368060312\n",
      "(9, 5, 4), σ: 3.3317337588224682\n",
      "(9, 6, 2), σ: 0.5595756932329841\n",
      "(9, 6, 3), σ: 3.3427194167188463\n",
      "(10, 4, 2), σ: 0.8695956655734514\n",
      "(10, 4, 3), σ: 0.10224391711451145\n",
      "(10, 4, 4), σ: 0.2553576547953333\n",
      "(10, 4, 5), σ: 0.8969262676696201\n",
      "(10, 4, 6), σ: 3.450894845210068\n",
      "(10, 5, 2), σ: 0.11960933253703701\n",
      "(10, 5, 3), σ: 0.3153405500508103\n",
      "(10, 5, 4), σ: 0.9021643464201516\n",
      "(10, 5, 5), σ: 3.5692605080877216\n",
      "(10, 6, 2), σ: 0.2334483852068471\n",
      "(10, 6, 3), σ: 0.8402805686969873\n",
      "(10, 6, 4), σ: 3.473932944098122\n"
     ]
    }
   ],
   "source": [
    "model_save_directory = \"model_6\"\n",
    "with gzip.open(model_save_directory+'/eval_inputs_combined', 'rb') as f:\n",
    "    eval_inputs_combined = pickle.load(f)\n",
    "with gzip.open(model_save_directory+'/eval_outputs_combined', 'rb') as f:\n",
    "    eval_outputs_combined = pickle.load(f)\n",
    "σ = evaluator.eval(\n",
    "        inputs=eval_inputs_combined,\n",
    "        outputs=eval_outputs_combined,\n",
    "        func=predict_and_evaluate,\n",
    "    )\n",
    "average_σ = sum(σ.values()) / len(σ) if σ else 0\n",
    "print(f\"Average σ: {average_σ}\")\n",
    "for key, value in σ.items():\n",
    "    print(f\"{key}, σ: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2a968a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average σ: 1.3369110484151927\n",
      "(9, 4, 2), σ: 0.16723566585624303\n",
      "(9, 4, 3), σ: 0.22076046554700068\n",
      "(9, 4, 4), σ: 0.8307825169388442\n",
      "(9, 4, 5), σ: 3.3139499956751277\n",
      "(9, 5, 2), σ: 0.19655960093129882\n",
      "(9, 5, 3), σ: 0.7839523695241959\n",
      "(9, 5, 4), σ: 3.3498014333850317\n",
      "(9, 6, 2), σ: 0.5853356126510034\n",
      "(9, 6, 3), σ: 3.418617047518573\n",
      "(10, 4, 2), σ: 0.870909757980427\n",
      "(10, 4, 3), σ: 0.09064671552202962\n",
      "(10, 4, 4), σ: 0.248986426422921\n",
      "(10, 4, 5), σ: 0.9003641177879229\n",
      "(10, 4, 6), σ: 3.4834119412376188\n",
      "(10, 5, 2), σ: 0.10344631007944895\n",
      "(10, 5, 3), σ: 0.30639232039130126\n",
      "(10, 5, 4), σ: 0.9126815335404459\n",
      "(10, 5, 5), σ: 3.6486358668172185\n",
      "(10, 6, 2), σ: 0.23215725476632176\n",
      "(10, 6, 3), σ: 0.8627622276023892\n",
      "(10, 6, 4), σ: 3.547742836543683\n"
     ]
    }
   ],
   "source": [
    "model_save_directory = \"model_15\"\n",
    "with gzip.open(model_save_directory+'/eval_inputs_combined', 'rb') as f:\n",
    "    eval_inputs_combined = pickle.load(f)\n",
    "with gzip.open(model_save_directory+'/eval_outputs_combined', 'rb') as f:\n",
    "    eval_outputs_combined = pickle.load(f)\n",
    "σ = evaluator.eval(\n",
    "        inputs=eval_inputs_combined,\n",
    "        outputs=eval_outputs_combined,\n",
    "        func=predict_and_evaluate,\n",
    "    )\n",
    "average_σ = sum(σ.values()) / len(σ) if σ else 0\n",
    "print(f\"Average σ: {average_σ}\")\n",
    "for key, value in σ.items():\n",
    "    print(f\"{key}, σ: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649f4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average σ: 1.3420547103612788\n",
      "(9, 4, 2), σ: 0.15783294877864124\n",
      "(9, 4, 3), σ: 0.21744654207274894\n",
      "(9, 4, 4), σ: 0.8197664727320247\n",
      "(9, 4, 5), σ: 3.327273094212319\n",
      "(9, 5, 2), σ: 0.18842748083876276\n",
      "(9, 5, 3), σ: 0.7767539430189144\n",
      "(9, 5, 4), σ: 3.415617086870581\n",
      "(9, 6, 2), σ: 0.5664792600907335\n",
      "(9, 6, 3), σ: 3.4397167269377222\n",
      "(10, 4, 2), σ: 0.8679689114665337\n",
      "(10, 4, 3), σ: 0.09507866568972716\n",
      "(10, 4, 4), σ: 0.25411240895024056\n",
      "(10, 4, 5), σ: 0.9323937398969352\n",
      "(10, 4, 6), σ: 3.49916901364459\n",
      "(10, 5, 2), σ: 0.10224265180293701\n",
      "(10, 5, 3), σ: 0.3045049337522387\n",
      "(10, 5, 4), σ: 0.9016705340662106\n",
      "(10, 5, 5), σ: 3.6325295136671096\n",
      "(10, 6, 2), σ: 0.2364360624962371\n",
      "(10, 6, 3), σ: 0.8787114986539757\n",
      "(10, 6, 4), σ: 3.56901742794767\n"
     ]
    }
   ],
   "source": [
    "model_save_directory = \"model_14\"\n",
    "with gzip.open(model_save_directory+'/eval_inputs_combined', 'rb') as f:\n",
    "    eval_inputs_combined = pickle.load(f)\n",
    "with gzip.open(model_save_directory+'/eval_outputs_combined', 'rb') as f:\n",
    "    eval_outputs_combined = pickle.load(f)\n",
    "σ = evaluator.eval(\n",
    "        inputs=eval_inputs_combined,\n",
    "        outputs=eval_outputs_combined,\n",
    "        func=predict_and_evaluate,\n",
    "    )\n",
    "average_σ = sum(σ.values()) / len(σ) if σ else 0\n",
    "print(f\"Average σ: {average_σ}\")\n",
    "for key, value in σ.items():\n",
    "    print(f\"{key}, σ: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebeaf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average σ: 1.3377592467696955\n",
      "(9, 4, 2), σ: 0.17365590323652508\n",
      "(9, 4, 3), σ: 0.2308594847306596\n",
      "(9, 4, 4), σ: 0.8252615940863484\n",
      "(9, 4, 5), σ: 3.43882251037085\n",
      "(9, 5, 2), σ: 0.21270508490425663\n",
      "(9, 5, 3), σ: 0.7694383907862355\n",
      "(9, 5, 4), σ: 3.397880503674985\n",
      "(9, 6, 2), σ: 0.5237900918288482\n",
      "(9, 6, 3), σ: 3.18749291171175\n",
      "(10, 4, 2), σ: 0.868691274230268\n",
      "(10, 4, 3), σ: 0.10688689304575737\n",
      "(10, 4, 4), σ: 0.25410796487092574\n",
      "(10, 4, 5), σ: 0.8722170234569177\n",
      "(10, 4, 6), σ: 3.521227586111585\n",
      "(10, 5, 2), σ: 0.12608340335449197\n",
      "(10, 5, 3), σ: 0.3363254893389921\n",
      "(10, 5, 4), σ: 0.9393224277845033\n",
      "(10, 5, 5), σ: 3.521088419941405\n",
      "(10, 6, 2), σ: 0.26776244883086564\n",
      "(10, 6, 3), σ: 0.8915959521932466\n",
      "(10, 6, 4), σ: 3.6277288236741905\n"
     ]
    }
   ],
   "source": [
    "model_save_directory = \"model_13\"\n",
    "with gzip.open(model_save_directory+'/eval_inputs_combined', 'rb') as f:\n",
    "    eval_inputs_combined = pickle.load(f)\n",
    "with gzip.open(model_save_directory+'/eval_outputs_combined', 'rb') as f:\n",
    "    eval_outputs_combined = pickle.load(f)\n",
    "σ = evaluator.eval(\n",
    "        inputs=eval_inputs_combined,\n",
    "        outputs=eval_outputs_combined,\n",
    "        func=predict_and_evaluate,\n",
    "    )\n",
    "average_σ = sum(σ.values()) / len(σ) if σ else 0\n",
    "print(f\"Average σ: {average_σ}\")\n",
    "for key, value in σ.items():\n",
    "    print(f\"{key}, σ: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
